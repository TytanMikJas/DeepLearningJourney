# Deep Learning Course Notebooks

This repository contains Jupyter notebooks from the "Mike X Cohen" Deep Learning course that I edited to display my skills.I have taken the course during the summer of 2023 to make a significant step in my journey into the fascinating world of Artificial Intelligence. I give full credit to Mike X Cohen for the invaluable content and knowledge shared in this course.

## A list of things I have learned:

✔️ - means that I have already completed the section in the course :)

### Concepts in Deep Learning: 
✔️
- Understanding artificial neural networks and their learning process.
- The role of Deep Learning in advancing science and knowledge.
- Comparing artificial "neurons" to biological neurons.

### Math, Numpy, PyTorch:
✔️
- Introduction to PyTorch and TensorFlow.
- Spectral theories in mathematics.
- Converting real-world data to numeric representation.
- Working with vectors and matrices, including transpose and dot product.
- Softmax, logarithms, entropy, cross-entropy, min/max, argmin/argmax, mean, and variance.
- Random sampling, reproducible randomness, and the t-test.
- Derivatives and their role in optimization, including the product and chain rules.

### Gradient Descent:
✔️
- Overview of gradient descent and its application in optimization.
- Handling local minima in gradient descent.
- Implementing gradient descent in 1D and 2D.
- Experimenting with gradient descent metaparameters.
- Managing vanishing and exploding gradients.
- Utilizing tangent notebooks for revision history.

### ANNs (Artificial Neural Networks):
✔️
- The architecture and geometry of perceptrons and ANNs.
- Mathematical aspects of ANNs, including forward and backward propagation.
- Implementing ANNs for regression and classification tasks.
- Examining the impact of hidden units and model depth.
- Comparing ANN visual representations and their understandability.

### Overfitting and Cross-Validation:
✔️
- Understanding overfitting and its implications.
- Using cross-validation to assess model performance and generalization.
- Manual and automated cross-validation techniques.
- Splitting data into training, development set, and test set.

### Regularization:
✔️
- Introduction to regularization and its methods.
- Using dropout and weight regularization (L1/L2) to prevent overfitting.
- Training models in mini-batches with equal batch sizes.

### Metaparameters (Activation, Optimizers):
✔️
- Exploring "metaparameters" and their role in model training.
- Normalizing data for improved performance.
- Understanding and implementing activation functions.
- Comparing various optimizers for model training.
- Selecting the right metaparameters for optimal performance.

### FFNs (Feed-Forward Networks):
✔️
- Building fully-connected and feedforward networks.
- Applying FFNs to classify digits and predict heart disease.
- Utilizing FFNs for missing data interpolation.

### More on Data:
✔️
- Understanding torch datasets and dataloaders.
- The impact of data size and network size on performance.
- Handling unbalanced designs, oversampling, noise augmentation, and feature augmentation.
- Saving and loading trained models and finding online datasets.

### Measuring Model Performance:
✔️
- Evaluating models using accuracy, precision, recall, and F1 score.
- Comparing model performance on different datasets.
- Analyzing computation time and performance variations.

### FFN Milestone Projects:
✔️
- Implementing projects involving complex adding machines, heart disease prediction, and missing data interpolation.

### Weights Inits and Investigations:
✔️
- Exploring weight initialization techniques and their effects.
- Comparing Xavier and Kaiming initializations.
- Analyzing changes in weights during learning and freezing weights.

### Autoencoders:
✔️
- Understanding autoencoders and their applications.
- Implementing denoising autoencoders and exploring the latent code of MNIST.

### Running Models on a GPU:
✔️
- Utilizing GPU for model training to improve performance.

### Convolution and Transformations:
✔️
- Understanding convolution and feature maps.
- Implementing convolution and pooling in code.
- Using transpose convolution and image transformations.

### Understand and Design CNNs:
✔️
- Building canonical CNN architecture for image classification.
- Analyzing feature map activations and internal parameters.
- Applying autoencoders for noise removal and occlusion handling.
- Creating custom loss functions for CNNs.

### CNN Milestone Projects:
✔️
- Importing and classifying CIFAR10, implementing CIFAR-autoencoder, and using CNNs for Psychometric functions.

### Transfer Learning:
✔️
- Understanding and applying transfer learning.
- Using pre-trained models and autoencoders for transfer learning.

### Style Transfer:

- Implementing style transfer algorithms for images.

### Generative Adversarian Networks (GANs):

- Building linear and CNN-based GANs for generating images.

### RNNs (Recurrent Neural Networks) and GRU/LSTM:

- Utilizing RNNs for sequence data processing and sine wave extrapolation.
- Implementing GRU and LSTM in PyTorch.

### Ethics on Deep Learning:

- Exploring ethical considerations and scenarios related to AI and deep learning.

This course has laid a solid foundation for my future endeavors in AI, and I am grateful to Mike X Cohen for sharing his expertise and insights. The journey in AI continues, and I look forward to further exploration and discoveries!
