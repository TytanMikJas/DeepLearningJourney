{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKGMTL6BcE4j4nlo3BzprJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TytanMikJas/DeepLearningJourney/blob/main/Gratuitously_complex_adding_machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The most complex adding machine ever! (my first independent DL project)\n",
        "The purpouse of my model is to predict the outcome of adding two numbers between -10 and 10. At first glance, the idea might sound idiotic, but my vision for the project is to allow my model to KNOW the answers, not calculate them. It's simmilar to how we solve 8+8. We do not actually calculate it we just... know it! :)"
      ],
      "metadata": {
        "id": "2mTH829iaHDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ikKZoG6_gWNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "# data and learning\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import matplotlib_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "# measuring\n",
        "import sklearn.metrics as skm"
      ],
      "metadata": {
        "id": "nRs1PKanbWrw"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and processing data"
      ],
      "metadata": {
        "id": "l0NGmHd_gTme"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "PK82Rum-Z-Ju"
      },
      "outputs": [],
      "source": [
        "# creating N number of 2 digit pairs (data) and their sum (labels)\n",
        "def create_data(N: int):\n",
        "    data = np.random.randint(-10, 10, size=(N, 2))\n",
        "    labels = np.sum(data,axis=1)\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INPUT NUMBER OF DATA TO GENERATE\n",
        "data, labels = create_data(2500)\n",
        "\n",
        "dataT = torch.tensor(data).float()\n",
        "labelsT = torch.tensor(labels).long()\n",
        "\n",
        "# dividing data into 80/10/10 sets (train, dev, test)\n",
        "train_data, dev_test_data, train_labels, dev_test_labels = train_test_split(dataT, labelsT, train_size=0.8)\n",
        "dev_data, test_data, dev_labels, test_labels = train_test_split(dev_test_data, dev_test_labels, train_size=0.5)\n",
        "\n",
        "# converting data into TensorDatasets and then DataLoaders\n",
        "train_data = TensorDataset(train_data, train_labels)\n",
        "dev_data  = TensorDataset(dev_data, dev_labels)\n",
        "test_data = TensorDataset(test_data, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16,shuffle=True,drop_last=True)\n",
        "dev_loader = DataLoader(dev_data, batch_size=dev_data.tensors[0].shape[0])\n",
        "test_loader = DataLoader(test_data, batch_size=test_data.tensors[0].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l4_E2WMcxCd",
        "outputId": "80d8af0f-6b93-4739-85be-304c6d72e48a"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 6, -1,  4,  ..., -1, -2, -3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Deep Learning model"
      ],
      "metadata": {
        "id": "jzFMJGiSgaoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create model, loss function and optimizer\n",
        "def create_model():\n",
        "\n",
        "    class AddingNet(nn.Module):\n",
        "\n",
        "        # class initialisation\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # input layer\n",
        "            self.input = nn.Linear(2,16)\n",
        "\n",
        "            # hidden layers\n",
        "            self.hidden = nn.Linear(16, 8)\n",
        "\n",
        "            # output layer\n",
        "            self.output = nn.Linear(8, 39)\n",
        "\n",
        "        # forward pass\n",
        "        def forward(self, x):\n",
        "\n",
        "            x = F.relu( self.input(x) )\n",
        "            x = F.relu( self.hidden(x) )\n",
        "\n",
        "            return self.output(x)\n",
        "\n",
        "    # model instance\n",
        "    net = AddingNet()\n",
        "\n",
        "    # loss function\n",
        "    lossfun = nn.CrossEntropyLoss()\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "    return net, lossfun, optimizer"
      ],
      "metadata": {
        "id": "_g8FhqGFgeiH"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainining function"
      ],
      "metadata": {
        "id": "WdXJ6WT3aOFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function2trainTheModel():\n",
        "\n",
        "    # number of epochs\n",
        "    numepochs = 100\n",
        "\n",
        "    # create a new model\n",
        "    net, lossfun, optimizer = create_model()\n",
        "\n",
        "\n",
        "    # initialize losses\n",
        "    losses    = torch.zeros(numepochs)\n",
        "    trainAcc  = []\n",
        "    devAcc   = []\n",
        "\n",
        "    # loop over epochs\n",
        "    for epochi in range(numepochs):\n",
        "\n",
        "        # loop over training data batches\n",
        "        batchAcc  = []\n",
        "        batchLoss = []\n",
        "        for X,y in train_loader:\n",
        "            print('1')\n",
        "            # forward pass and loss\n",
        "            yHat = net(X)\n",
        "            print('1.5')\n",
        "            loss = lossfun(yHat,y)\n",
        "            print('2')\n",
        "\n",
        "            # backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print('3')\n",
        "\n",
        "            # loss from this batch\n",
        "            batchLoss.append(loss.item())\n",
        "\n",
        "            # compute accuracy\n",
        "            matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
        "            matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
        "            accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
        "            batchAcc.append( accuracyPct )               # add to list of accuracies\n",
        "            print('4')\n",
        "\n",
        "\n",
        "        # average accuracy and losses over batches\n",
        "        trainAcc.append( np.mean(batchAcc) )\n",
        "        losses[epochi] = np.mean(batchLoss)\n",
        "\n",
        "        # test accuracy\n",
        "        X,y = next(iter(dev_loader)) # extract X,y from test dataloader\n",
        "        with torch.no_grad(): # deactivates autograd\n",
        "            yHat = net(X)\n",
        "\n",
        "        # compare the following really long line of code to the training accuracy lines\n",
        "        devAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "\n",
        "    return trainAcc, devAcc, losses, net"
      ],
      "metadata": {
        "id": "whm_hRyya-S4"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the model"
      ],
      "metadata": {
        "id": "JBCrQFnUgwMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "\n",
        "    # build and train the model\n",
        "    trainAcc, devAcc, losses, net = function2trainTheModel()\n",
        "\n",
        "    # visualization starts here\n",
        "    fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
        "\n",
        "    ax[0].plot(losses)\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "    ax[0].set_ylabel('Loss')\n",
        "    ax[0].set_ylim([0,3])\n",
        "    ax[0].set_title('Model loss')\n",
        "\n",
        "    ax[1].plot(trainAcc,label='Train')\n",
        "    ax[1].plot(devAcc,label='Test')\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "    ax[1].set_ylabel('Accuracy (%)')\n",
        "    ax[1].set_ylim([10,100])\n",
        "    ax[1].set_title(f'Final model test accuracy: {devAcc[-1]:.2f}%')\n",
        "    ax[1].legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "QRiDEjtjgyRJ",
        "outputId": "8ef7a118-6d22-4ccb-bc90-367ef3a1fd02"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-b123eab293eb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# build and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainAcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevAcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction2trainTheModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# visualization starts here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-2d58881de7d9>\u001b[0m in \u001b[0;36mfunction2trainTheModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0myHat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1.5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myHat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target -3 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JEBIE SIE TO ZE PREZIWDUJEMY od -20 do 20 i zapewne cross entrophy loss daje argmax no i przewiduje label -20, a nie ma takiego argmax"
      ],
      "metadata": {
        "id": "CxaLzmdnk5g1"
      }
    }
  ]
}